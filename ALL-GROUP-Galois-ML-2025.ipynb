{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a15fbad-c4a4-4d7a-afd5-7672d2b37a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLUMNS in file:\n",
      "['coeffs', 'height', 'disc', 'galois_label', 'math_galois_notation']\n",
      "Rows total: 1686353\n",
      "Detected label column: math_galois_notation\n",
      "Rows (canonical classes only): 1686353\n",
      "Class counts: {'S7': 1559957, 'A7': 56997, 'PSL(3,2)': 40977, 'C7 ⋊ C3': 24457, 'D7': 2163, 'C7 ⋊ C6': 1550, 'C7': 252}\n",
      "Train size: 1011811;  Test size: 674542 (test fraction 0.40)\n",
      "Fold 1: balanced accuracy = 0.7996\n",
      "Fold 2: balanced accuracy = 0.8237\n",
      "Fold 3: balanced accuracy = 0.8002\n",
      "Fold 4: balanced accuracy = 0.8128\n",
      "Fold 5: balanced accuracy = 0.8154\n",
      "[CV on train] Balanced accuracy: 0.8103 ± 0.0093\n",
      "Weight map (train): {'A7': 0.655, 'C7': 9.858, 'C7 ⋊ C3': 1.0, 'C7 ⋊ C6': 3.972, 'D7': 3.362, 'PSL(3,2)': 0.773, 'S7': 0.125}\n",
      "\n",
      "[Test 40%] Balanced accuracy: 0.8236\n",
      "\n",
      "[Test] Classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          S7     0.9992    0.9802    0.9896    623983\n",
      "          A7     0.6999    0.8744    0.7775     22799\n",
      "    PSL(3,2)     0.5401    0.5631    0.5513     16391\n",
      "     C7 ⋊ C3     0.9986    0.9958    0.9972      9783\n",
      "          D7     0.1425    0.7838    0.2412       865\n",
      "     C7 ⋊ C6     0.1742    0.6371    0.2736       620\n",
      "          C7     0.7705    0.9307    0.8430       101\n",
      "\n",
      "    accuracy                         0.9661    674542\n",
      "   macro avg     0.6179    0.8236    0.6676    674542\n",
      "weighted avg     0.9761    0.9661    0.9703    674542\n",
      "\n",
      "\n",
      "[Test] Confusion matrix (rows=true, cols=pred):\n",
      "              S7     A7  PSL(3,2)  C7 ⋊ C3    D7  C7 ⋊ C6  C7\n",
      "S7        611602   1572      5059        8  3904     1828  10\n",
      "A7            93  19936      2744        0    25        0   1\n",
      "PSL(3,2)     137   6974      9229        4    42        0   5\n",
      "C7 ⋊ C3        0      0        25     9742     9        0   7\n",
      "D7           116      0        21        1   678       44   5\n",
      "C7 ⋊ C6      120      1         6        0    98      395   0\n",
      "C7             0      0         5        1     1        0  94\n",
      "Saved model to: /Users/jurimezini/Library/CloudStorage/Dropbox/Sage_Galois7/AIMS-Galois-7/models_all_noinv_60_40/allgroups_noinv_60_40_full.joblib\n",
      "Config and feature names saved in: /Users/jurimezini/Library/CloudStorage/Dropbox/Sage_Galois7/AIMS-Galois-7/models_all_noinv_60_40\n"
     ]
    }
   ],
   "source": [
    "# === ALL-GROUP ML (coeffs + disc only, NO invariants) — 60/40 split ===\n",
    "# Sage/Jupyter compatible\n",
    "\n",
    "import os, ast, json, hashlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from builtins import int as pyint, float as pyfloat  # avoid Sage Integer/Real issues\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, StratifiedKFold, GroupKFold\n",
    ")\n",
    "\n",
    "# StratifiedGroupKFold may not exist in all sklearn versions\n",
    "try:\n",
    "    from sklearn.model_selection import StratifiedGroupKFold\n",
    "    HAS_SGF = True\n",
    "except Exception:\n",
    "    HAS_SGF = False\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.metrics import classification_report, balanced_accuracy_score, confusion_matrix\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "from joblib import dump\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "CSV_PATH = \"/Users/jurimezini/Library/CloudStorage/Dropbox/Sage_Galois7/AIMS-Galois-7/AIMS-7.csv\"\n",
    "OUT_DIR  = os.path.join(os.path.dirname(CSV_PATH), \"models_all_noinv_60_40\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "RANDOM   = pyint(42)\n",
    "\n",
    "# For this run we want the FULL dataset (no quick subsampling)\n",
    "QUICK    = False\n",
    "N_QUICK  = pyint(120000)   # only used if QUICK=True for testing\n",
    "\n",
    "# Feature toggles\n",
    "USE_COEFFS = True\n",
    "USE_DISC   = True      # disc_sign + disc_log10abs\n",
    "USE_JINV   = False     # <--- NO invariants here\n",
    "\n",
    "# Split controls\n",
    "TEST_FRACTION     = pyfloat(0.40)  # 60/40 split\n",
    "USE_GROUPED_SPLIT = False          # family-aware split off for now\n",
    "\n",
    "# Canonical label space for ALL groups\n",
    "CANON_ALL = [\"S7\", \"A7\", \"PSL(3,2)\", \"C7 ⋊ C3\", \"D7\", \"C7 ⋊ C6\", \"C7\"]\n",
    "\n",
    "# Normalize some group names if needed\n",
    "NORMALIZE_GROUP = {\n",
    "    \"L(3,2\": \"PSL(3,2)\",\n",
    "    \"L(3,2)\": \"PSL(3,2)\",\n",
    "    \"7:2\": \"D7\",\n",
    "    \"7:6\": \"C7 ⋊ C6\",\n",
    "}\n",
    "\n",
    "# Map 7T* codes to math notation if we ever use galois_label\n",
    "MAP_7T = {\n",
    "    \"7T7\": \"S7\",\n",
    "    \"7T6\": \"A7\",\n",
    "    \"7T5\": \"PSL(3,2)\",\n",
    "    \"7T3\": \"C7 ⋊ C3\",\n",
    "    \"7T2\": \"D7\",\n",
    "    \"7T4\": \"C7 ⋊ C6\",\n",
    "    \"7T1\": \"C7\",\n",
    "}\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "def signed_log10(X):\n",
    "    \"\"\"\n",
    "    Elementwise sgn(x)*log10(1+|x|); kept for completeness if you later add j-invariants.\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, dtype=np.float64)\n",
    "    return np.sign(X) * np.log10(1.0 + np.abs(X))\n",
    "\n",
    "def class_weight_samples(y_vec, power=0.5):\n",
    "    \"\"\"\n",
    "    Per-sample weights ~ (median_count / class_count)^power to soften imbalance.\n",
    "    \"\"\"\n",
    "    power = float(power)\n",
    "    vals, counts = np.unique(y_vec, return_counts=True)\n",
    "    med = np.median(counts)\n",
    "    wmap = {c: (med / cnt) ** power for c, cnt in zip(vals, counts)}\n",
    "    return np.array([wmap[c] for c in y_vec], dtype=np.float64), wmap\n",
    "\n",
    "def parse_coeffs_tuple(cell):\n",
    "    \"\"\"\n",
    "    Accept '(...)', '[...]', tuple/list/ndarray; return list[int] of length 8.\n",
    "    For AIMS-7.csv, 'coeffs' is like \"[1, 1, -1, 0, 1, -1, -1, 1]\".\n",
    "    \"\"\"\n",
    "    if isinstance(cell, (list, tuple, np.ndarray)):\n",
    "        v = list(cell)\n",
    "    elif isinstance(cell, str):\n",
    "        v = ast.literal_eval(cell)\n",
    "    else:\n",
    "        v = cell\n",
    "    if v is None or len(v) != 8:\n",
    "        raise ValueError(f\"Expected 8 coefficients, got {v}\")\n",
    "    return [int(x) for x in v]\n",
    "\n",
    "def stable_tuple_hash(row8):\n",
    "    \"\"\"\n",
    "    Deterministic hash for an 8-int coefficient tuple (for grouping).\n",
    "    \"\"\"\n",
    "    b = (\",\".join(map(str, row8))).encode(\"utf-8\")\n",
    "    return int(hashlib.sha1(b).hexdigest()[:12], 16)  # 48-bit int\n",
    "\n",
    "# ---------------- Read CSV (robust) ----------------\n",
    "df = pd.read_csv(CSV_PATH, low_memory=False)\n",
    "df.columns = [str(c).strip() for c in df.columns]\n",
    "print(\"COLUMNS in file:\")\n",
    "print(df.columns.tolist())\n",
    "print(\"Rows total:\", len(df))\n",
    "\n",
    "# Prefer explicit a0..a7; expand tuple only if needed\n",
    "coeff_cols = [f\"a{i}\" for i in range(8)]\n",
    "have_all_coeffs = all(c in df.columns for c in coeff_cols)\n",
    "\n",
    "if not have_all_coeffs:\n",
    "    tuple_col = None\n",
    "    for alt in [\"coeffs_tuple_(a0..a7)\", \"coeffs_tuple\", \"coeffs\"]:\n",
    "        if alt in df.columns:\n",
    "            tuple_col = alt\n",
    "            break\n",
    "    if tuple_col is None:\n",
    "        raise ValueError(\"Need coefficients: either a0..a7 or a tuple column must be present.\")\n",
    "\n",
    "    df[tuple_col] = df[tuple_col].apply(parse_coeffs_tuple)\n",
    "    coeff_mat = np.vstack(df[tuple_col].to_numpy()).astype(np.int64)\n",
    "    df_coeffs = pd.DataFrame(coeff_mat, columns=coeff_cols, copy=False)\n",
    "    df = pd.concat(\n",
    "        [df.drop(columns=[tuple_col]).reset_index(drop=True),\n",
    "         df_coeffs.reset_index(drop=True)],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "# Drop leftover textual 'coeffs' column to avoid confusion\n",
    "if \"coeffs\" in df.columns and \"coeffs\" not in coeff_cols:\n",
    "    try:\n",
    "        df = df.drop(columns=[\"coeffs\"])\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# ---- Add/repair discriminant-derived features if only raw 'disc' exists ----\n",
    "if USE_DISC:\n",
    "    if \"disc_sign\" not in df.columns and \"disc\" in df.columns:\n",
    "        df[\"disc_sign\"] = np.sign(pd.to_numeric(df[\"disc\"], errors=\"coerce\")).astype(\"float64\")\n",
    "    if \"disc_log10abs\" not in df.columns and \"disc\" in df.columns:\n",
    "        df[\"disc_log10abs\"] = np.log10(\n",
    "            1.0 + np.abs(pd.to_numeric(df[\"disc\"], errors=\"coerce\"))\n",
    "        )\n",
    "\n",
    "# ---- Force numeric types where appropriate ----\n",
    "numeric_cols = [c for c in\n",
    "                [\"disc\", \"disc_sign\", \"disc_log10abs\",\n",
    "                 \"j0\", \"j1\", \"j2\", \"j3\", \"j4\"] + coeff_cols\n",
    "                if c in df.columns]\n",
    "\n",
    "for c in numeric_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# ---- Resolve/normalize labels robustly ----\n",
    "label_col = None\n",
    "for cand in [\"group\", \"math_galois_notation\", \"galois_label\"]:\n",
    "    if cand in df.columns:\n",
    "        label_col = cand\n",
    "        break\n",
    "\n",
    "if label_col is None:\n",
    "    raise ValueError(f\"No label column found; have columns: {list(df.columns)}\")\n",
    "\n",
    "if label_col == \"galois_label\":\n",
    "    # Map 7T* codes to math notation\n",
    "    y_raw = df[\"galois_label\"].astype(str).str.strip().map(MAP_7T)\n",
    "else:\n",
    "    # For AIMS-7.csv this will be 'math_galois_notation'\n",
    "    y_raw = df[label_col].astype(str).str.strip()\n",
    "\n",
    "y_raw = y_raw.replace(NORMALIZE_GROUP)\n",
    "\n",
    "# Keep only canonical 7 classes (drop anything weird/typo)\n",
    "mask = y_raw.isin(CANON_ALL)\n",
    "df   = df.loc[mask].reset_index(drop=True)\n",
    "y    = y_raw.loc[mask].to_numpy(dtype=object)\n",
    "\n",
    "print(\"Detected label column:\", label_col)\n",
    "print(\"Rows (canonical classes only):\", len(df))\n",
    "print(\"Class counts:\", {c: int((y == c).sum()) for c in CANON_ALL})\n",
    "\n",
    "# ---- Optional quick subset (OFF for this full-data run) ----\n",
    "if QUICK and len(y) > N_QUICK:\n",
    "    rng = check_random_state(0)\n",
    "    idx = rng.choice(len(y), size=N_QUICK, replace=False)\n",
    "    df, y = df.iloc[idx].reset_index(drop=True), y[idx]\n",
    "    print(f\"[QUICK] Using subset of {len(y)} rows\")\n",
    "\n",
    "# ---------------- Feature assembly ----------------\n",
    "logj_cols  = [c for c in [\"j0\", \"j1\", \"j2\", \"j3\", \"j4\"] if USE_JINV and c in df.columns]\n",
    "disc_cols  = [c for c in [\"disc_sign\", \"disc_log10abs\"] if USE_DISC and c in df.columns]\n",
    "use_coeffs = [c for c in coeff_cols if USE_COEFFS and c in df.columns]\n",
    "\n",
    "transformers = []\n",
    "\n",
    "# (logj_cols will be empty since USE_JINV=False, but kept for future)\n",
    "if logj_cols:\n",
    "    transformers.append(\n",
    "        (\"logj\",\n",
    "         Pipeline([\n",
    "             (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "             (\"log\", FunctionTransformer(signed_log10, feature_names_out=\"one-to-one\")),\n",
    "         ]),\n",
    "         logj_cols)\n",
    "    )\n",
    "\n",
    "if disc_cols:\n",
    "    transformers.append(\n",
    "        (\"disc\",\n",
    "         SimpleImputer(strategy=\"median\"),\n",
    "         disc_cols)\n",
    "    )\n",
    "\n",
    "if use_coeffs:\n",
    "    transformers.append(\n",
    "        (\"coeffs\",\n",
    "         Pipeline([\n",
    "             (\"imp\", SimpleImputer(strategy=\"constant\", fill_value=pyint(0))),\n",
    "         ]),\n",
    "         use_coeffs)\n",
    "    )\n",
    "\n",
    "pre = ColumnTransformer(\n",
    "    transformers=transformers,\n",
    "    remainder=\"drop\",\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "clf = HistGradientBoostingClassifier(\n",
    "    loss=\"log_loss\",\n",
    "    learning_rate=0.08,\n",
    "    max_iter=500,\n",
    "    max_leaf_nodes=31,\n",
    "    min_samples_leaf=30,\n",
    "    l2_regularization=1e-4,\n",
    "    early_stopping=True,\n",
    "    n_iter_no_change=20,\n",
    "    random_state=RANDOM,\n",
    ")\n",
    "\n",
    "pipe = Pipeline([(\"pre\", pre), (\"clf\", clf)])\n",
    "\n",
    "# ---------------- Build groups for optional leakage-resistant split ----------------\n",
    "if USE_GROUPED_SPLIT:\n",
    "    coeff_mat = df[[f\"a{i}\" for i in range(8)]].to_numpy(dtype=np.int64)\n",
    "    g_hash = np.array([stable_tuple_hash(row) for row in coeff_mat], dtype=np.int64)\n",
    "    height_bin = (\n",
    "        pd.to_numeric(df.get(\"height\", 0), errors=\"coerce\")\n",
    "        .fillna(0).astype(np.int64).to_numpy()\n",
    "    )\n",
    "    groups = (height_bin.astype(np.int64) * 10**6 + (g_hash % 10**6)).astype(np.int64)\n",
    "else:\n",
    "    groups = None\n",
    "\n",
    "# ---------------- External 60/40 hold-out ----------------\n",
    "if USE_GROUPED_SPLIT:\n",
    "    from sklearn.model_selection import GroupShuffleSplit\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=float(TEST_FRACTION),\n",
    "                            random_state=RANDOM)\n",
    "    tr_idx, te_idx = next(gss.split(df, y, groups=groups))\n",
    "    X_tr, X_te = df.iloc[tr_idx], df.iloc[te_idx]\n",
    "    y_tr, y_te = y[tr_idx], y[te_idx]\n",
    "    g_tr, g_te = groups[tr_idx], groups[te_idx]\n",
    "else:\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        df, y,\n",
    "        test_size=float(TEST_FRACTION),\n",
    "        random_state=RANDOM,\n",
    "        stratify=y\n",
    "    )\n",
    "    g_tr = g_te = None\n",
    "\n",
    "print(f\"Train size: {len(y_tr)};  Test size: {len(y_te)} (test fraction {TEST_FRACTION:.2f})\")\n",
    "\n",
    "# ---------------- 5-fold CV on TRAIN ----------------\n",
    "cv_scores = []\n",
    "\n",
    "if USE_GROUPED_SPLIT and HAS_SGF:\n",
    "    cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=RANDOM)\n",
    "    splitter = cv.split(X_tr, y_tr, groups=g_tr)\n",
    "elif USE_GROUPED_SPLIT:\n",
    "    cv = GroupKFold(n_splits=5)\n",
    "    splitter = cv.split(X_tr, groups=g_tr)\n",
    "else:\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM)\n",
    "    splitter = cv.split(X_tr, y_tr)\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(splitter, 1):\n",
    "    y_tr_fold = y_tr[tr_idx] if isinstance(y_tr, np.ndarray) else np.asarray(y_tr)[tr_idx]\n",
    "    w_fold, _ = class_weight_samples(y_tr_fold, power=0.5)\n",
    "    pipe.fit(X_tr.iloc[tr_idx], y_tr_fold, clf__sample_weight=w_fold)\n",
    "    pred_va = pipe.predict(X_tr.iloc[va_idx])\n",
    "    ba = balanced_accuracy_score(y_tr[va_idx], pred_va)\n",
    "    cv_scores.append(ba)\n",
    "    print(f\"Fold {fold}: balanced accuracy = {ba:.4f}\")\n",
    "\n",
    "print(f\"[CV on train] Balanced accuracy: {np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}\")\n",
    "\n",
    "# ---------------- Final fit on ALL TRAIN; evaluate on TEST ----------------\n",
    "w_tr_all, wmap = class_weight_samples(y_tr, power=0.5)\n",
    "print(\"Weight map (train):\", {k: round(v, 3) for k, v in wmap.items()})\n",
    "\n",
    "pipe.fit(X_tr, y_tr, clf__sample_weight=w_tr_all)\n",
    "\n",
    "pred_te = pipe.predict(X_te)\n",
    "print(f\"\\n[Test {int(TEST_FRACTION*100)}%] Balanced accuracy: \"\n",
    "      f\"{balanced_accuracy_score(y_te, pred_te):.4f}\\n\")\n",
    "\n",
    "print(\"[Test] Classification report\")\n",
    "print(classification_report(y_te, pred_te, labels=CANON_ALL,\n",
    "                            digits=4, zero_division=0))\n",
    "\n",
    "print(\"\\n[Test] Confusion matrix (rows=true, cols=pred):\")\n",
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(y_te, pred_te, labels=CANON_ALL),\n",
    "    index=CANON_ALL,\n",
    "    columns=CANON_ALL\n",
    ")\n",
    "print(cm)\n",
    "\n",
    "# ---------------- Save model + config ----------------\n",
    "model_path = os.path.join(OUT_DIR, \"allgroups_noinv_60_40_full.joblib\")\n",
    "dump(pipe, model_path)\n",
    "print(\"Saved model to:\", model_path)\n",
    "\n",
    "config = {\n",
    "    \"label_space\": [str(c) for c in CANON_ALL],\n",
    "    \"columns_in\": [str(c) for c in df.columns],\n",
    "    \"feature_blocks\": {\n",
    "        \"coeffs\": [str(c) for c in use_coeffs],\n",
    "        \"disc\":   [str(c) for c in disc_cols],\n",
    "        \"j_invariants\": [str(c) for c in logj_cols],\n",
    "    },\n",
    "    \"quick_mode\": bool(QUICK),\n",
    "    \"test_fraction\": float(TEST_FRACTION),\n",
    "    \"grouped_split\": bool(USE_GROUPED_SPLIT),\n",
    "}\n",
    "with open(os.path.join(OUT_DIR, \"config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(config, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "try:\n",
    "    feats_out = pipe.named_steps[\"pre\"].get_feature_names_out()\n",
    "    with open(os.path.join(OUT_DIR, \"features_out.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        for nm in feats_out:\n",
    "            f.write(f\"{nm}\\n\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(\"Config and feature names saved in:\", OUT_DIR)\n",
    "\n",
    "# ---------------- One-off prediction helper ----------------\n",
    "def predict_one(coeffs8=None,\n",
    "                disc_sign=None,\n",
    "                disc_log10abs=None):\n",
    "    \"\"\"\n",
    "    Predict a Galois group for a single septic using the trained model.\n",
    "    Inputs:\n",
    "      - coeffs8: list/tuple of 8 integers [a0,...,a7]\n",
    "      - disc_sign: sign of discriminant (-1,0,1) or None\n",
    "      - disc_log10abs: log10(1+|disc|) or None\n",
    "    \"\"\"\n",
    "    row = {}\n",
    "\n",
    "    if USE_COEFFS:\n",
    "        if coeffs8 is None or len(coeffs8) != 8:\n",
    "            raise ValueError(\"coeffs8 must be length-8 when USE_COEFFS=True\")\n",
    "        for i, v in enumerate(coeffs8):\n",
    "            row[f\"a{i}\"] = int(v)\n",
    "\n",
    "    if USE_DISC:\n",
    "        row[\"disc_sign\"]     = None if disc_sign is None else int(disc_sign)\n",
    "        row[\"disc_log10abs\"] = None if disc_log10abs is None else float(disc_log10abs)\n",
    "\n",
    "    # Build a one-row DataFrame with exactly the same input columns used in training\n",
    "    cols_in = pipe.named_steps[\"pre\"].feature_names_in_\n",
    "    X1 = pd.DataFrame([row], columns=cols_in)\n",
    "    return pipe.predict(X1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30c4f3a5-441b-475a-b960-678c7aa09a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLUMNS in file:\n",
      "['coeffs', 'height', 'disc', 'galois_label', 'math_galois_notation']\n",
      "Rows total: 1686353\n",
      "Detected label column: math_galois_notation\n",
      "Rows (canonical classes only): 1686353\n",
      "Class counts: {'S7': 1559957, 'A7': 56997, 'PSL(3,2)': 40977, 'C7 ⋊ C3': 24457, 'D7': 2163, 'C7 ⋊ C6': 1550, 'C7': 252}\n",
      "Train size: 1011811;  Test size: 674542 (test fraction 0.40)\n",
      "Fold 1: balanced accuracy = 0.7491\n",
      "Fold 2: balanced accuracy = 0.7596\n",
      "Fold 3: balanced accuracy = 0.7357\n",
      "Fold 4: balanced accuracy = 0.7625\n",
      "Fold 5: balanced accuracy = 0.7559\n",
      "[CV on train] Balanced accuracy: 0.7526 ± 0.0095\n",
      "Weight map (train): {'A7': 0.655, 'C7': 9.858, 'C7 ⋊ C3': 1.0, 'C7 ⋊ C6': 3.972, 'D7': 3.362, 'PSL(3,2)': 0.773, 'S7': 0.125}\n",
      "\n",
      "[Test 40%] Balanced accuracy: 0.7726\n",
      "\n",
      "[Test] Classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          S7     0.9934    0.9653    0.9792    623983\n",
      "          A7     0.5314    0.8270    0.6470     22799\n",
      "    PSL(3,2)     0.4344    0.4552    0.4446     16391\n",
      "     C7 ⋊ C3     0.9982    0.9952    0.9967      9783\n",
      "          D7     0.1307    0.6439    0.2173       865\n",
      "     C7 ⋊ C6     0.2787    0.6306    0.3866       620\n",
      "          C7     0.6250    0.8911    0.7347       101\n",
      "\n",
      "    accuracy                         0.9479    674542\n",
      "   macro avg     0.5703    0.7726    0.6294    674542\n",
      "weighted avg     0.9625    0.9479    0.9536    674542\n",
      "\n",
      "\n",
      "[Test] Confusion matrix (rows=true, cols=pred):\n",
      "              S7     A7  PSL(3,2)  C7 ⋊ C3    D7  C7 ⋊ C6  C7\n",
      "S7        602337   9506      8084       10  3100      923  23\n",
      "A7          2444  18854      1337        0   131       29   4\n",
      "PSL(3,2)    1431   7067      7462        1   387       30  13\n",
      "C7 ⋊ C3        0      0        16     9736    25        1   5\n",
      "D7            57     16       194        3   557       29   9\n",
      "C7 ⋊ C6       52     37        81        2    57      391   0\n",
      "C7             0      0         4        2     5        0  90\n",
      "Saved model to: /Users/jurimezini/Library/CloudStorage/Dropbox/Sage_Galois7/AIMS-Galois-7/models_all_coeffs_only_60_40/allgroups_coeffs_only_60_40_full.joblib\n",
      "Config and feature names saved in: /Users/jurimezini/Library/CloudStorage/Dropbox/Sage_Galois7/AIMS-Galois-7/models_all_coeffs_only_60_40\n"
     ]
    }
   ],
   "source": [
    "# === ALL-GROUP ML (coefficients only, NO disc, NO invariants) — 60/40 split ===\n",
    "# Sage/Jupyter compatible\n",
    "\n",
    "import os, ast, json, hashlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from builtins import int as pyint, float as pyfloat  # avoid Sage Integer/Real issues\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, StratifiedKFold, GroupKFold\n",
    ")\n",
    "\n",
    "# StratifiedGroupKFold may not exist in all sklearn versions\n",
    "try:\n",
    "    from sklearn.model_selection import StratifiedGroupKFold\n",
    "    HAS_SGF = True\n",
    "except Exception:\n",
    "    HAS_SGF = False\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.metrics import classification_report, balanced_accuracy_score, confusion_matrix\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "from joblib import dump\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "CSV_PATH = \"/Users/jurimezini/Library/CloudStorage/Dropbox/Sage_Galois7/AIMS-Galois-7/AIMS-7.csv\"\n",
    "OUT_DIR  = os.path.join(os.path.dirname(CSV_PATH), \"models_all_coeffs_only_60_40\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "RANDOM   = pyint(42)\n",
    "\n",
    "# For this run we want the FULL dataset (no quick subsampling)\n",
    "QUICK    = False\n",
    "N_QUICK  = pyint(120000)   # only used if QUICK=True for testing\n",
    "\n",
    "# Feature toggles\n",
    "USE_COEFFS = True\n",
    "USE_DISC   = False     # <--- NO discriminant features\n",
    "USE_JINV   = False     # <--- NO invariants\n",
    "\n",
    "# Split controls\n",
    "TEST_FRACTION     = pyfloat(0.40)  # 60/40 split\n",
    "USE_GROUPED_SPLIT = False          # family-aware split off for now\n",
    "\n",
    "# Canonical label space for ALL groups\n",
    "CANON_ALL = [\"S7\", \"A7\", \"PSL(3,2)\", \"C7 ⋊ C3\", \"D7\", \"C7 ⋊ C6\", \"C7\"]\n",
    "\n",
    "# Normalize some group names if needed\n",
    "NORMALIZE_GROUP = {\n",
    "    \"L(3,2\": \"PSL(3,2)\",\n",
    "    \"L(3,2)\": \"PSL(3,2)\",\n",
    "    \"7:2\": \"D7\",\n",
    "    \"7:6\": \"C7 ⋊ C6\",\n",
    "}\n",
    "\n",
    "# Map 7T* codes to math notation if we ever use galois_label\n",
    "MAP_7T = {\n",
    "    \"7T7\": \"S7\",\n",
    "    \"7T6\": \"A7\",\n",
    "    \"7T5\": \"PSL(3,2)\",\n",
    "    \"7T3\": \"C7 ⋊ C3\",\n",
    "    \"7T2\": \"D7\",\n",
    "    \"7T4\": \"C7 ⋊ C6\",\n",
    "    \"7T1\": \"C7\",\n",
    "}\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "def signed_log10(X):\n",
    "    X = np.asarray(X, dtype=np.float64)\n",
    "    return np.sign(X) * np.log10(1.0 + np.abs(X))\n",
    "\n",
    "def class_weight_samples(y_vec, power=0.5):\n",
    "    power = float(power)\n",
    "    vals, counts = np.unique(y_vec, return_counts=True)\n",
    "    med = np.median(counts)\n",
    "    wmap = {c: (med / cnt) ** power for c, cnt in zip(vals, counts)}\n",
    "    return np.array([wmap[c] for c in y_vec], dtype=np.float64), wmap\n",
    "\n",
    "def parse_coeffs_tuple(cell):\n",
    "    \"\"\"\n",
    "    Accept '(...)', '[...]', tuple/list/ndarray; return list[int] of length 8.\n",
    "    For AIMS-7.csv, 'coeffs' is like \"[1, 1, -1, 0, 1, -1, -1, 1]\".\n",
    "    \"\"\"\n",
    "    if isinstance(cell, (list, tuple, np.ndarray)):\n",
    "        v = list(cell)\n",
    "    elif isinstance(cell, str):\n",
    "        v = ast.literal_eval(cell)\n",
    "    else:\n",
    "        v = cell\n",
    "    if v is None or len(v) != 8:\n",
    "        raise ValueError(f\"Expected 8 coefficients, got {v}\")\n",
    "    return [int(x) for x in v]\n",
    "\n",
    "def stable_tuple_hash(row8):\n",
    "    b = (\",\".join(map(str, row8))).encode(\"utf-8\")\n",
    "    return int(hashlib.sha1(b).hexdigest()[:12], 16)  # 48-bit int\n",
    "\n",
    "# ---------------- Read CSV (robust) ----------------\n",
    "df = pd.read_csv(CSV_PATH, low_memory=False)\n",
    "df.columns = [str(c).strip() for c in df.columns]\n",
    "print(\"COLUMNS in file:\")\n",
    "print(df.columns.tolist())\n",
    "print(\"Rows total:\", len(df))\n",
    "\n",
    "# Prefer explicit a0..a7; expand tuple only if needed\n",
    "coeff_cols = [f\"a{i}\" for i in range(8)]\n",
    "have_all_coeffs = all(c in df.columns for c in coeff_cols)\n",
    "\n",
    "if not have_all_coeffs:\n",
    "    tuple_col = None\n",
    "    for alt in [\"coeffs_tuple_(a0..a7)\", \"coeffs_tuple\", \"coeffs\"]:\n",
    "        if alt in df.columns:\n",
    "            tuple_col = alt\n",
    "            break\n",
    "    if tuple_col is None:\n",
    "        raise ValueError(\"Need coefficients: either a0..a7 or a tuple column must be present.\")\n",
    "\n",
    "    df[tuple_col] = df[tuple_col].apply(parse_coeffs_tuple)\n",
    "    coeff_mat = np.vstack(df[tuple_col].to_numpy()).astype(np.int64)\n",
    "    df_coeffs = pd.DataFrame(coeff_mat, columns=coeff_cols, copy=False)\n",
    "    df = pd.concat(\n",
    "        [df.drop(columns=[tuple_col]).reset_index(drop=True),\n",
    "         df_coeffs.reset_index(drop=True)],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "# Drop leftover textual 'coeffs' column to avoid confusion\n",
    "if \"coeffs\" in df.columns and \"coeffs\" not in coeff_cols:\n",
    "    try:\n",
    "        df = df.drop(columns=[\"coeffs\"])\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# ---- Force numeric types where appropriate ----\n",
    "numeric_cols = [c for c in\n",
    "                [\"disc\", \"disc_sign\", \"disc_log10abs\",\n",
    "                 \"j0\", \"j1\", \"j2\", \"j3\", \"j4\"] + coeff_cols\n",
    "                if c in df.columns]\n",
    "\n",
    "for c in numeric_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# ---- Resolve/normalize labels robustly ----\n",
    "label_col = None\n",
    "for cand in [\"group\", \"math_galois_notation\", \"galois_label\"]:\n",
    "    if cand in df.columns:\n",
    "        label_col = cand\n",
    "        break\n",
    "\n",
    "if label_col is None:\n",
    "    raise ValueError(f\"No label column found; have columns: {list(df.columns)}\")\n",
    "\n",
    "if label_col == \"galois_label\":\n",
    "    y_raw = df[\"galois_label\"].astype(str).str.strip().map(MAP_7T)\n",
    "else:\n",
    "    y_raw = df[label_col].astype(str).str.strip()\n",
    "\n",
    "y_raw = y_raw.replace(NORMALIZE_GROUP)\n",
    "\n",
    "# Keep only canonical 7 classes (drop anything weird/typo)\n",
    "mask = y_raw.isin(CANON_ALL)\n",
    "df   = df.loc[mask].reset_index(drop=True)\n",
    "y    = y_raw.loc[mask].to_numpy(dtype=object)\n",
    "\n",
    "print(\"Detected label column:\", label_col)\n",
    "print(\"Rows (canonical classes only):\", len(df))\n",
    "print(\"Class counts:\", {c: int((y == c).sum()) for c in CANON_ALL})\n",
    "\n",
    "# ---- Optional quick subset (OFF for this full-data run) ----\n",
    "if QUICK and len(y) > N_QUICK:\n",
    "    rng = check_random_state(0)\n",
    "    idx = rng.choice(len(y), size=N_QUICK, replace=False)\n",
    "    df, y = df.iloc[idx].reset_index(drop=True), y[idx]\n",
    "    print(f\"[QUICK] Using subset of {len(y)} rows\")\n",
    "\n",
    "# ---------------- Feature assembly ----------------\n",
    "logj_cols  = [c for c in [\"j0\", \"j1\", \"j2\", \"j3\", \"j4\"] if USE_JINV and c in df.columns]\n",
    "disc_cols  = []  # USE_DISC=False, so empty\n",
    "use_coeffs = [c for c in coeff_cols if USE_COEFFS and c in df.columns]\n",
    "\n",
    "transformers = []\n",
    "\n",
    "if disc_cols:\n",
    "    transformers.append(\n",
    "        (\"disc\",\n",
    "         SimpleImputer(strategy=\"median\"),\n",
    "         disc_cols)\n",
    "    )\n",
    "\n",
    "if use_coeffs:\n",
    "    transformers.append(\n",
    "        (\"coeffs\",\n",
    "         Pipeline([\n",
    "             (\"imp\", SimpleImputer(strategy=\"constant\", fill_value=pyint(0))),\n",
    "         ]),\n",
    "         use_coeffs)\n",
    "    )\n",
    "\n",
    "pre = ColumnTransformer(\n",
    "    transformers=transformers,\n",
    "    remainder=\"drop\",\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "clf = HistGradientBoostingClassifier(\n",
    "    loss=\"log_loss\",\n",
    "    learning_rate=0.08,\n",
    "    max_iter=500,\n",
    "    max_leaf_nodes=31,\n",
    "    min_samples_leaf=30,\n",
    "    l2_regularization=1e-4,\n",
    "    early_stopping=True,\n",
    "    n_iter_no_change=20,\n",
    "    random_state=RANDOM,\n",
    ")\n",
    "\n",
    "pipe = Pipeline([(\"pre\", pre), (\"clf\", clf)])\n",
    "\n",
    "# ---------------- Build groups for optional leakage-resistant split ----------------\n",
    "if USE_GROUPED_SPLIT:\n",
    "    coeff_mat = df[[f\"a{i}\" for i in range(8)]].to_numpy(dtype=np.int64)\n",
    "    g_hash = np.array([stable_tuple_hash(row) for row in coeff_mat], dtype=np.int64)\n",
    "    height_bin = (\n",
    "        pd.to_numeric(df.get(\"height\", 0), errors=\"coerce\")\n",
    "        .fillna(0).astype(np.int64).to_numpy()\n",
    "    )\n",
    "    groups = (height_bin.astype(np.int64) * 10**6 + (g_hash % 10**6)).astype(np.int64)\n",
    "else:\n",
    "    groups = None\n",
    "\n",
    "# ---------------- External 60/40 hold-out ----------------\n",
    "if USE_GROUPED_SPLIT:\n",
    "    from sklearn.model_selection import GroupShuffleSplit\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=float(TEST_FRACTION),\n",
    "                            random_state=RANDOM)\n",
    "    tr_idx, te_idx = next(gss.split(df, y, groups=groups))\n",
    "    X_tr, X_te = df.iloc[tr_idx], df.iloc[te_idx]\n",
    "    y_tr, y_te = y[tr_idx], y[te_idx]\n",
    "    g_tr, g_te = groups[tr_idx], groups[te_idx]\n",
    "else:\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        df, y,\n",
    "        test_size=float(TEST_FRACTION),\n",
    "        random_state=RANDOM,\n",
    "        stratify=y\n",
    "    )\n",
    "    g_tr = g_te = None\n",
    "\n",
    "print(f\"Train size: {len(y_tr)};  Test size: {len(y_te)} (test fraction {TEST_FRACTION:.2f})\")\n",
    "\n",
    "# ---------------- 5-fold CV on TRAIN ----------------\n",
    "cv_scores = []\n",
    "\n",
    "if USE_GROUPED_SPLIT and HAS_SGF:\n",
    "    cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=RANDOM)\n",
    "    splitter = cv.split(X_tr, y_tr, groups=g_tr)\n",
    "elif USE_GROUPED_SPLIT:\n",
    "    cv = GroupKFold(n_splits=5)\n",
    "    splitter = cv.split(X_tr, groups=g_tr)\n",
    "else:\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM)\n",
    "    splitter = cv.split(X_tr, y_tr)\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(splitter, 1):\n",
    "    y_tr_fold = y_tr[tr_idx] if isinstance(y_tr, np.ndarray) else np.asarray(y_tr)[tr_idx]\n",
    "    w_fold, _ = class_weight_samples(y_tr_fold, power=0.5)\n",
    "    pipe.fit(X_tr.iloc[tr_idx], y_tr_fold, clf__sample_weight=w_fold)\n",
    "    pred_va = pipe.predict(X_tr.iloc[va_idx])\n",
    "    ba = balanced_accuracy_score(y_tr[va_idx], pred_va)\n",
    "    cv_scores.append(ba)\n",
    "    print(f\"Fold {fold}: balanced accuracy = {ba:.4f}\")\n",
    "\n",
    "print(f\"[CV on train] Balanced accuracy: {np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}\")\n",
    "\n",
    "# ---------------- Final fit on ALL TRAIN; evaluate on TEST ----------------\n",
    "w_tr_all, wmap = class_weight_samples(y_tr, power=0.5)\n",
    "print(\"Weight map (train):\", {k: round(v, 3) for k, v in wmap.items()})\n",
    "\n",
    "pipe.fit(X_tr, y_tr, clf__sample_weight=w_tr_all)\n",
    "\n",
    "pred_te = pipe.predict(X_te)\n",
    "print(f\"\\n[Test {int(TEST_FRACTION*100)}%] Balanced accuracy: \"\n",
    "      f\"{balanced_accuracy_score(y_te, pred_te):.4f}\\n\")\n",
    "\n",
    "print(\"[Test] Classification report\")\n",
    "print(classification_report(y_te, pred_te, labels=CANON_ALL,\n",
    "                            digits=4, zero_division=0))\n",
    "\n",
    "print(\"\\n[Test] Confusion matrix (rows=true, cols=pred):\")\n",
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(y_te, pred_te, labels=CANON_ALL),\n",
    "    index=CANON_ALL,\n",
    "    columns=CANON_ALL\n",
    ")\n",
    "print(cm)\n",
    "\n",
    "# ---------------- Save model + config ----------------\n",
    "model_path = os.path.join(OUT_DIR, \"allgroups_coeffs_only_60_40_full.joblib\")\n",
    "dump(pipe, model_path)\n",
    "print(\"Saved model to:\", model_path)\n",
    "\n",
    "config = {\n",
    "    \"label_space\": [str(c) for c in CANON_ALL],\n",
    "    \"columns_in\": [str(c) for c in df.columns],\n",
    "    \"feature_blocks\": {\n",
    "        \"coeffs\": [str(c) for c in use_coeffs],\n",
    "        \"disc\":   [],          # no disc features\n",
    "        \"j_invariants\": [],    # no invariants\n",
    "    },\n",
    "    \"quick_mode\": bool(QUICK),\n",
    "    \"test_fraction\": float(TEST_FRACTION),\n",
    "    \"grouped_split\": bool(USE_GROUPED_SPLIT),\n",
    "}\n",
    "with open(os.path.join(OUT_DIR, \"config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(config, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "try:\n",
    "    feats_out = pipe.named_steps[\"pre\"].get_feature_names_out()\n",
    "    with open(os.path.join(OUT_DIR, \"features_out.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        for nm in feats_out:\n",
    "            f.write(f\"{nm}\\n\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(\"Config and feature names saved in:\", OUT_DIR)\n",
    "\n",
    "# ---------------- One-off prediction helper ----------------\n",
    "def predict_one(coeffs8=None):\n",
    "    \"\"\"\n",
    "    Predict a Galois group for a single septic using the trained model.\n",
    "    Input:\n",
    "      - coeffs8: list/tuple of 8 integers [a0,...,a7]\n",
    "    \"\"\"\n",
    "    row = {}\n",
    "\n",
    "    if USE_COEFFS:\n",
    "        if coeffs8 is None or len(coeffs8) != 8:\n",
    "            raise ValueError(\"coeffs8 must be length-8 when USE_COEFFS=True\")\n",
    "        for i, v in enumerate(coeffs8):\n",
    "            row[f\"a{i}\"] = int(v)\n",
    "\n",
    "    cols_in = pipe.named_steps[\"pre\"].feature_names_in_\n",
    "    X1 = pd.DataFrame([row], columns=cols_in)\n",
    "    return pipe.predict(X1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7f228a-629b-4497-a8f4-462a1fcf90dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 10.4",
   "language": "sage",
   "name": "sagemath-10.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
